{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRA (Finetuning)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-deepmind/gemma/blob/main/colabs/qlora_finetuning.ipynb)\n",
    "\n",
    "This is an example on fine-tuning Gemma with QLoRA (Quantized Low-Rank Adaptation). It builds on the [LoRA finetuning](https://gemma-llm.readthedocs.io/en/latest/lora_finetuning.html) tutorial, so it's recommended to read that first.\n",
    "\n",
    "QLoRA combines the parameter-efficient fine-tuning of LoRA with model weight quantization, reducing memory requirements significantly while maintaining performance. This allows for fine-tuning larger models on consumer hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gemma"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Common imports\nimport os\nimport optax\nimport treescope\n\n# Gemma imports\nfrom kauldron import kd\nfrom gemma import gm"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Jax does not utilize the full GPU memory, but this can be overwritten. See [GPU memory allocation](https://docs.jax.dev/en/latest/gpu_memory_allocation.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config updates\n",
    "\n",
    "Like regular LoRA, QLoRA requires 3 main changes to the trainer configuration. The key difference is using `QLoRA` instead of `LoRA` and specifying a quantization method.\n",
    "\n",
    "For an end-to-end example, see [qlora.py](https://github.com/google-deepmind/gemma/tree/main/examples/qlora.py) config."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model\n",
    "\n",
    "Wrap the model in the `gm.nn.QLoRA`. This will apply model surgery to replace all the linear and compatible layers with quantized versions that have LoRA adapters."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "model = gm.nn.QLoRA(\n    rank=8,  # QLoRA typically uses higher rank than standard LoRA\n    quant_method=gm.peft.QuantizationMethod.INT4,  # 4-bit quantization\n    model=gm.nn.Gemma3_4B(\n        tokens=\"batch.input\",\n        text_only=True,  # Important: Make sure the model is text-only\n    ),\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, this uses the [`gemma.peft`](https://github.com/google-deepmind/gemma/blob/main/gemma/peft) mini-library to perform model surgery with quantization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Checkpoint\n",
    "\n",
    "Just like with LoRA, wrap the init transform in a `gm.ckpts.SkipLoRA`. The wrapper is required because the param structure with QLoRA is different from the original model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Standard approach using SkipLoRA\n# Use WithRngKeys to ensure it has the right RNG keys\nfrom kauldron import ckpts\n\n# Create a wrapper that adds the right RNG keys\nclass WithRngKeys(ckpts.AbstractPartialLoader):\n    def __init__(self, wrapped):\n        self.wrapped = wrapped\n        \n    def transform(self, state):\n        # Apply the wrapped transform\n        state = self.wrapped.transform(state)\n        return state\n\ninit_transform = WithRngKeys(\n    wrapped=gm.ckpts.SkipLoRA(\n        wrapped=gm.ckpts.LoadCheckpoint(\n            path=gm.ckpts.CheckpointPath.GEMMA3_4B_IT,\n        ),\n    ),\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you're loading the weights directly with `gm.ckpts.load_params`, you can use the `peft.split_params` and `peft.merge_params` instead, similar to the LoRA approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Optimizer\n",
    "\n",
    "Similar to LoRA, add a mask to the optimizer so only the LoRA weights are trained. With QLoRA, it's common to use a lower learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = kd.optim.partial_updates(\n",
    "    optax.adafactor(learning_rate=1e-4),  # Lower learning rate for QLoRA\n",
    "    # We only optimize the LoRA weights. The rest of the model is frozen.\n",
    "    mask=kd.optim.select(\"lora\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pipeline\n",
    "\n",
    "The data pipeline setup is identical to the regular LoRA approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = gm.text.Gemma3Tokenizer()\n",
    "\n",
    "tokenizer.encode('This is an example sentence', add_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = kd.data.py.Tfds(\n",
    "    name='mtnt/en-fr',\n",
    "    split='train',\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    "    transforms=[\n",
    "        # Create the model inputs/targets/loss_mask.\n",
    "        gm.data.Seq2SeqTask(\n",
    "            # Select which field from the dataset to use.\n",
    "            # https://www.tensorflow.org/datasets/catalog/mtnt\n",
    "            in_prompt='src',\n",
    "            in_response='dst',\n",
    "            # Output batch is {'input': ..., 'target': ..., 'loss_mask': ...}\n",
    "            out_input='input',\n",
    "            out_target='target',\n",
    "            out_target_mask='loss_mask',\n",
    "            tokenizer=tokenizer,\n",
    "            # Padding parameters\n",
    "            max_length=200,\n",
    "            truncate=True,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "ex = ds[0]\n",
    "\n",
    "treescope.show(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can decode an example from the batch to inspect the model input and check it is properly formatted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.decode(ex['input'][0])\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer\n",
    "\n",
    "Create the trainer, reusing the `model`, `init_transform` and `optimizer` defined above:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "trainer = kd.train.Trainer(\n    seed=42,  # The seed of enlightenment\n    workdir='/tmp/ckpts',\n    # Dataset\n    train_ds=ds,\n    # Model\n    model=model,\n    init_transform=init_transform,\n    # Training parameters\n    num_train_steps=500,\n    train_losses={\n        \"loss\": kd.losses.SoftmaxCrossEntropyWithIntLabels(\n            logits=\"preds.logits\",\n            labels=\"batch.target\",\n            mask=\"batch.loss_mask\",\n        ),\n    },\n    optimizer=optimizer,\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Let's create a more targeted debugging approach based on quantization patterns\nimport jax\nimport jax.numpy as jnp\nimport logging\nlogging.basicConfig(level=logging.INFO)\n\n# First, let's understand how the quantization init works\nprint(\"Creating standard model (without LoRA) for reference\")\nstandard_model = gm.nn.Gemma3_4B(tokens=\"batch.input\", text_only=True)\n\nprint(\"Creating QLoRA model\")\nqlora_model = gm.nn.QLoRA(\n    rank=8,\n    quant_method=gm.peft.QuantizationMethod.INT4,\n    model=gm.nn.Gemma3_4B(tokens=\"batch.input\", text_only=True),\n)\n\nprint(\"Step 1: Loading checkpoint directly for reference\")\ntry:\n    # Load original parameters first for reference\n    original_params = gm.ckpts.load_params(\n        path=gm.ckpts.CheckpointPath.GEMMA3_4B_IT,\n    )\n    print(\"Successfully loaded original checkpoint\")\nexcept Exception as e:\n    print(f\"Error loading original checkpoint: {str(e)}\")\n    raise\n\nprint(\"Step 2: Initializing the QLoRA model\")\ntry:\n    # Create dummy data for initialization\n    dummy_ids = tokenizer.encode(\"Test\", add_bos=True)\n    dummy_input = jnp.array([dummy_ids])\n    \n    # Initialize the QLoRA model\n    variables = qlora_model.init(\n        jax.random.PRNGKey(42),\n        tokens=dummy_input,\n    )\n    params = variables[\"params\"]\n    print(\"Successfully initialized QLoRA model\")\nexcept Exception as e:\n    print(f\"Error initializing QLoRA model: {str(e)}\")\n    raise\n\nprint(\"Step 3: Separating QLoRA parameters\")\ntry:\n    # Split the QLoRA parameters\n    base_params, lora_params = gm.peft.split_params(params)\n    print(\"Successfully split parameters\")\nexcept Exception as e:\n    print(f\"Error splitting parameters: {str(e)}\")\n    raise\n\nprint(\"Step 4: Restoring base parameters while preserving structure\")\ntry:\n    # Get the structure of original params that matches our base params\n    restored_params = {}\n    for key, subtree in original_params.items():\n        if key in base_params:\n            restored_params[key] = subtree\n    \n    print(\"Successfully prepared parameters for merging\")\nexcept Exception as e:\n    print(f\"Error preparing parameters: {str(e)}\")\n    raise\n\nprint(\"Step 5: Merging with LoRA parameters\")\ntry:\n    # Merge the parameters\n    final_params = gm.peft.merge_params(restored_params, lora_params)\n    print(\"Successfully merged parameters\")\nexcept Exception as e:\n    print(f\"Error merging parameters: {str(e)}\")\n    raise\n\nprint(\"All debug steps completed!\")\nprint(\"\\nNote: This debugging doesn't actually use the init_transform with SkipLoRA,\")\nprint(\"but demonstrates how the manual parameter loading and merging should work.\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Manual approach for QLoRA initialization and training\nimport jax\nimport dataclasses\nfrom kauldron import random\n\n# Create proper RNG streams to use for module initialization\nrng_streams = random.RngStreams(seed=42)\nrngs = rng_streams.init_rngs()\n\n# Initialize model with dummy data\ndummy_ids = tokenizer.encode(\"Test\", add_bos=True)\ndummy_input = jnp.array([dummy_ids])\n\n# Initialize QLoRA model\nvariables = model.init(\n    rngs,  # Use the proper RNGs with all required streams\n    tokens=dummy_input,\n)\nparams = variables[\"params\"]\n\n# Split parameters\noriginal_params, lora_params = gm.peft.split_params(params)\n\n# Load original parameters\ncheckpoint_params = gm.ckpts.load_params(\n    path=gm.ckpts.CheckpointPath.GEMMA3_4B_IT,\n)\n\n# Get common keys\nrestored_params = {}\nfor key in original_params:\n    if key in checkpoint_params:\n        restored_params[key] = checkpoint_params[key]\n\n# Merge with LoRA parameters\nfinal_params = gm.peft.merge_params(restored_params, lora_params)\n\n# Create a custom init_transform that returns our prepared parameters\n# and preserves the RNGs needed for proper initialization\n@dataclasses.dataclass\nclass CustomInitTransform:\n    def transform(self, state):\n        # Return state with our custom parameters\n        return state.replace(params=final_params)\n\n# Create trainer with our custom init_transform and full rng_streams\nmanual_trainer = kd.train.Trainer(\n    seed=42,  # Use the same seed for consistent behavior\n    workdir='/tmp/ckpts',\n    train_ds=ds,\n    model=model,\n    init_transform=CustomInitTransform(),\n    num_train_steps=500,\n    train_losses={\n        \"loss\": kd.losses.SoftmaxCrossEntropyWithIntLabels(\n            logits=\"preds.logits\",\n            labels=\"batch.target\",\n            mask=\"batch.loss_mask\",\n        ),\n    },\n    optimizer=optimizer,\n    rng_streams=rng_streams,  # Explicitly provide RNG streams\n)\n\n# Try training with our manual approach\ntry:\n    state, aux = manual_trainer.train()\n    print(\"Training succeeded with manual parameter loading!\")\nexcept Exception as e:\n    print(f\"Error during training with manual loading: {str(e)}\")\n    # Print RNG keys to help diagnose issues\n    print(f\"Available RNG keys: {rngs.keys()}\")\n    raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Let's test our fine-tuned model with a sample input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = gm.text.ChatSampler(\n",
    "    model=model,\n",
    "    params=state.params,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test a sentence, using the same formatting used during fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.chat('I\\'m feeling happy today!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QLoRA vs LoRA: Memory Comparison\n",
    "\n",
    "QLoRA offers significant memory savings compared to regular LoRA, especially for larger models. A rough comparison:\n",
    "\n",
    "| Model Size | Full Fine-tuning | LoRA | QLoRA (INT4) |\n",
    "|------------|-----------------|------|-------------|\n",
    "| 4B         | ~8 GB           | ~5 GB | ~3 GB        |\n",
    "| 12B        | ~24 GB          | ~10 GB| ~5 GB        |\n",
    "\n",
    "These are approximate values and actual memory usage depends on sequence length, batch size, and specific hardware/framework implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "QLoRA provides an excellent balance between memory efficiency and fine-tuning performance. By quantizing the frozen base model weights, we can dramatically reduce memory usage while still maintaining the benefits of parameter-efficient fine-tuning.\n",
    "\n",
    "This approach is particularly valuable when working with larger models or when computation resources are limited."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}