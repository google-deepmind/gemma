{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRA (Finetuning)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-deepmind/gemma/blob/main/colabs/qlora_finetuning.ipynb)\n",
    "\n",
    "This is an example on fine-tuning Gemma with QLoRA (Quantized Low-Rank Adaptation). It builds on the [LoRA finetuning](https://gemma-llm.readthedocs.io/en/latest/lora_finetuning.html) tutorial, so it's recommended to read that first.\n",
    "\n",
    "QLoRA combines the parameter-efficient fine-tuning of LoRA with model weight quantization, reducing memory requirements significantly while maintaining performance. This allows for fine-tuning larger models on consumer hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import os\n",
    "import optax\n",
    "import treescope\n",
    "\n",
    "# Gemma imports\n",
    "from kauldron import kd\n",
    "from gemma import gm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Jax does not utilize the full GPU memory, but this can be overwritten. See [GPU memory allocation](https://docs.jax.dev/en/latest/gpu_memory_allocation.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config updates\n",
    "\n",
    "Like regular LoRA, QLoRA requires 3 main changes to the trainer configuration. The key difference is using `QLoRA` instead of `LoRA` and specifying a quantization method.\n",
    "\n",
    "For an end-to-end example, see [qlora.py](https://github.com/google-deepmind/gemma/tree/main/examples/qlora.py) config."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model\n",
    "\n",
    "Wrap the model in the `gm.nn.QLoRA`. This will apply model surgery to replace all the linear and compatible layers with quantized versions that have LoRA adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = gm.nn.QLoRA(\n",
    "    rank=8,  # QLoRA typically uses higher rank than standard LoRA\n",
    "    quant_method=gm.peft.QuantizationMethod.INT4,  # 4-bit quantization\n",
    "    model=gm.nn.Gemma3_4B(\n",
    "        tokens=\"batch.input\",  # This is critical - matches how tokens are named in the batch\n",
    "        text_only=True,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, this uses the [`gemma.peft`](https://github.com/google-deepmind/gemma/blob/main/gemma/peft) mini-library to perform model surgery with quantization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Checkpoint\n",
    "\n",
    "Just like with LoRA, wrap the init transform in a `gm.ckpts.SkipLoRA`. The wrapper is required because the param structure with QLoRA is different from the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint\n",
    "init_transform = gm.ckpts.SkipLoRA(\n",
    "    wrapped=gm.ckpts.LoadCheckpoint(\n",
    "        path=gm.ckpts.CheckpointPath.GEMMA3_4B_IT,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you're loading the weights directly with `gm.ckpts.load_params`, you can use the `peft.split_params` and `peft.merge_params` instead, similar to the LoRA approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Optimizer\n",
    "\n",
    "Similar to LoRA, add a mask to the optimizer so only the LoRA weights are trained. With QLoRA, it's common to use a lower learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = kd.optim.partial_updates(\n",
    "    optax.adafactor(learning_rate=1e-4),  # Lower learning rate for QLoRA\n",
    "    # We only optimize the LoRA weights. The rest of the model is frozen.\n",
    "    mask=kd.optim.select(\"lora\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pipeline\n",
    "\n",
    "The data pipeline setup is identical to the regular LoRA approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = gm.text.Gemma3Tokenizer()\n",
    "\n",
    "tokenizer.encode('This is an example sentence', add_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "ds = kd.data.py.Tfds(\n",
    "    name='mtnt/en-fr',\n",
    "    split='train',\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    "    transforms=[\n",
    "        # Create the model inputs/targets/loss_mask.\n",
    "        gm.data.Seq2SeqTask(\n",
    "            # Select which field from the dataset to use.\n",
    "            # https://www.tensorflow.org/datasets/catalog/mtnt\n",
    "            in_prompt='src',\n",
    "            in_response='dst',\n",
    "            # Output batch is {'input': ..., 'target': ..., 'loss_mask': ...}\n",
    "            out_input='input',\n",
    "            out_target='target',\n",
    "            out_target_mask='loss_mask',\n",
    "            tokenizer=tokenizer,\n",
    "            # Padding parameters\n",
    "            max_length=200,\n",
    "            truncate=True,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "ex = ds[0]\n",
    "\n",
    "treescope.show(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can decode an example from the batch to inspect the model input and check it is properly formatted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.decode(ex['input'][0])\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer\n",
    "\n",
    "Create the trainer, reusing the `model`, `init_transform` and `optimizer` defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = kd.train.Trainer(\n",
    "    seed=42,\n",
    "    workdir='/tmp/ckpts',\n",
    "    # Dataset\n",
    "    train_ds=ds,\n",
    "    # Model\n",
    "    model=model,\n",
    "    init_transform=init_transform,\n",
    "    # Training parameters\n",
    "    num_train_steps=500,\n",
    "    train_losses={\n",
    "        \"loss\": kd.losses.SoftmaxCrossEntropyWithIntLabels(\n",
    "            logits=\"preds.logits\",\n",
    "            labels=\"batch.target\",\n",
    "            mask=\"batch.loss_mask\",\n",
    "        ),\n",
    "    },\n",
    "    optimizer=optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training can be launched with the `.train()` method. Note that the trainer and model are immutable, so they do not store state or parameters. Instead, the state containing the trained parameters is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "state, aux = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "After training, we can evaluate the model by sampling a prompt to translate from English to French:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sampler using the trained parameters\n",
    "sampler = gm.text.ChatSampler(\n",
    "    model=model,\n",
    "    params=state.params,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the sampler with a new prompt\n",
    "sampler.chat(\"I'm feeling happy today!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How QLoRA Works\n",
    "\n",
    "QLoRA combines two key techniques:\n",
    "\n",
    "1. **Quantization**: Reduces the precision of the model's weights (to 4-bit in our case)\n",
    "2. **Low-Rank Adaptation**: Adds small trainable matrices to adapt the quantized model\n",
    "\n",
    "### Under the Hood\n",
    "\n",
    "The implementation handles two distinct phases differently:\n",
    "\n",
    "- **Training**: During initialization, quantized weights and LoRA adapters are created\n",
    "- **Evaluation**: Special handling bypasses adapter creation when not in initialization mode\n",
    "\n",
    "This approach avoids RNG key issues during evaluation while maintaining proper parameter structure during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QLoRA vs LoRA: Memory Comparison\n",
    "\n",
    "QLoRA offers significant memory savings compared to regular LoRA, especially for larger models. A rough comparison:\n",
    "\n",
    "| Model Size | Full Fine-tuning | LoRA | QLoRA (INT4) |\n",
    "|------------|-----------------|------|-------------|\n",
    "| 4B         | ~8 GB           | ~5 GB | ~3 GB        |\n",
    "| 12B        | ~24 GB          | ~10 GB| ~5 GB        |\n",
    "\n",
    "These are approximate values and actual memory usage depends on sequence length, batch size, and specific hardware/framework implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading QLoRA Weights\n",
    "\n",
    "QLoRA weights can be saved and loaded similar to LoRA weights. Here's an example of how to save just the adapter weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to save only the LoRA weights\n",
    "import os\n",
    "\n",
    "# Split model parameters to isolate LoRA weights\n",
    "_, lora_params = gm.peft.split_params(state.params)\n",
    "\n",
    "# Save only the LoRA parameters (much smaller size)\n",
    "save_path = \"/tmp/my_qlora_weights\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "gm.ckpts.save_params(\n",
    "    params=lora_params,\n",
    "    path=save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to load the saved weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of loading the saved QLoRA weights for inference\n",
    "import jax\n",
    "\n",
    "# Load base model parameters\n",
    "base_params = gm.ckpts.load_params(\n",
    "    path=gm.ckpts.CheckpointPath.GEMMA3_4B_IT,\n",
    ")\n",
    "\n",
    "# Load LoRA parameters\n",
    "lora_params = gm.ckpts.load_params(\n",
    "    path=\"/tmp/my_qlora_weights\",\n",
    ")\n",
    "\n",
    "# Merge parameters for inference\n",
    "merged_params = gm.peft.merge_params(base_params, lora_params)\n",
    "\n",
    "# Create a new sampler with the merged parameters\n",
    "inference_sampler = gm.text.ChatSampler(\n",
    "    model=model,\n",
    "    params=merged_params,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "QLoRA provides an excellent balance between memory efficiency and fine-tuning performance. By quantizing the frozen base model weights, we can dramatically reduce memory usage while still maintaining the benefits of parameter-efficient fine-tuning.\n",
    "\n",
    "This approach is particularly valuable when working with larger models or when computation resources are limited."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}