{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRA (Finetuning)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-deepmind/gemma/blob/main/colabs/qlora_finetuning.ipynb)\n",
    "\n",
    "This is an example on fine-tuning Gemma with QLoRA (Quantized Low-Rank Adaptation). It builds on the [LoRA finetuning](https://gemma-llm.readthedocs.io/en/latest/lora_finetuning.html) tutorial, so it's recommended to read that first.\n",
    "\n",
    "QLoRA combines the parameter-efficient fine-tuning of LoRA with model weight quantization, reducing memory requirements significantly while maintaining performance. This allows for fine-tuning larger models on consumer hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gemma"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Common imports\nimport os\nimport optax\nimport treescope\n\n# Gemma imports\nfrom kauldron import kd\nfrom gemma import gm"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Jax does not utilize the full GPU memory, but this can be overwritten. See [GPU memory allocation](https://docs.jax.dev/en/latest/gpu_memory_allocation.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config updates\n",
    "\n",
    "Like regular LoRA, QLoRA requires 3 main changes to the trainer configuration. The key difference is using `QLoRA` instead of `LoRA` and specifying a quantization method.\n",
    "\n",
    "For an end-to-end example, see [qlora.py](https://github.com/google-deepmind/gemma/tree/main/examples/qlora.py) config."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model\n",
    "\n",
    "Wrap the model in the `gm.nn.QLoRA`. This will apply model surgery to replace all the linear and compatible layers with quantized versions that have LoRA adapters."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "model = gm.nn.QLoRA(\n    rank=8,  # QLoRA typically uses higher rank than standard LoRA\n    quant_method=gm.peft.QuantizationMethod.INT4,  # 4-bit quantization\n    model=gm.nn.Gemma3_4B(tokens=\"batch.input\", text_only=True),\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, this uses the [`gemma.peft`](https://github.com/google-deepmind/gemma/blob/main/gemma/peft) mini-library to perform model surgery with quantization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Checkpoint\n",
    "\n",
    "Just like with LoRA, wrap the init transform in a `gm.ckpts.SkipLoRA`. The wrapper is required because the param structure with QLoRA is different from the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_transform = gm.ckpts.SkipLoRA(\n",
    "    wrapped=gm.ckpts.LoadCheckpoint(\n",
    "        path=gm.ckpts.CheckpointPath.GEMMA3_4B_IT,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you're loading the weights directly with `gm.ckpts.load_params`, you can use the `peft.split_params` and `peft.merge_params` instead, similar to the LoRA approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Optimizer\n",
    "\n",
    "Similar to LoRA, add a mask to the optimizer so only the LoRA weights are trained. With QLoRA, it's common to use a lower learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = kd.optim.partial_updates(\n",
    "    optax.adafactor(learning_rate=1e-4),  # Lower learning rate for QLoRA\n",
    "    # We only optimize the LoRA weights. The rest of the model is frozen.\n",
    "    mask=kd.optim.select(\"lora\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pipeline\n",
    "\n",
    "The data pipeline setup is identical to the regular LoRA approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = gm.text.Gemma3Tokenizer()\n",
    "\n",
    "tokenizer.encode('This is an example sentence', add_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = kd.data.py.Tfds(\n",
    "    name='mtnt/en-fr',\n",
    "    split='train',\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    "    transforms=[\n",
    "        # Create the model inputs/targets/loss_mask.\n",
    "        gm.data.Seq2SeqTask(\n",
    "            # Select which field from the dataset to use.\n",
    "            # https://www.tensorflow.org/datasets/catalog/mtnt\n",
    "            in_prompt='src',\n",
    "            in_response='dst',\n",
    "            # Output batch is {'input': ..., 'target': ..., 'loss_mask': ...}\n",
    "            out_input='input',\n",
    "            out_target='target',\n",
    "            out_target_mask='loss_mask',\n",
    "            tokenizer=tokenizer,\n",
    "            # Padding parameters\n",
    "            max_length=200,\n",
    "            truncate=True,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "ex = ds[0]\n",
    "\n",
    "treescope.show(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can decode an example from the batch to inspect the model input and check it is properly formatted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.decode(ex['input'][0])\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer\n",
    "\n",
    "Create the trainer, reusing the `model`, `init_transform` and `optimizer` defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = kd.train.Trainer(\n",
    "    seed=42,  # The seed of enlightenment\n",
    "    workdir='/tmp/ckpts',\n",
    "    # Dataset\n",
    "    train_ds=ds,\n",
    "    # Model\n",
    "    model=model,\n",
    "    init_transform=init_transform,\n",
    "    # Training parameters\n",
    "    num_train_steps=500,\n",
    "    train_losses={\n",
    "        \"loss\": kd.losses.SoftmaxCrossEntropyWithIntLabels(\n",
    "            logits=\"preds.logits\",\n",
    "            labels=\"batch.target\",\n",
    "            mask=\"batch.loss_mask\",\n",
    "        ),\n",
    "    },\n",
    "    optimizer=optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training can be launched with the `.train()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, aux = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Let's test our fine-tuned model with a sample input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = gm.text.ChatSampler(\n",
    "    model=model,\n",
    "    params=state.params,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test a sentence, using the same formatting used during fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.chat('I\\'m feeling happy today!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QLoRA vs LoRA: Memory Comparison\n",
    "\n",
    "QLoRA offers significant memory savings compared to regular LoRA, especially for larger models. A rough comparison:\n",
    "\n",
    "| Model Size | Full Fine-tuning | LoRA | QLoRA (INT4) |\n",
    "|------------|-----------------|------|-------------|\n",
    "| 4B         | ~8 GB           | ~5 GB | ~3 GB        |\n",
    "| 12B        | ~24 GB          | ~10 GB| ~5 GB        |\n",
    "\n",
    "These are approximate values and actual memory usage depends on sequence length, batch size, and specific hardware/framework implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "QLoRA provides an excellent balance between memory efficiency and fine-tuning performance. By quantizing the frozen base model weights, we can dramatically reduce memory usage while still maintaining the benefits of parameter-efficient fine-tuning.\n",
    "\n",
    "This approach is particularly valuable when working with larger models or when computation resources are limited."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}