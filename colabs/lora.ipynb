{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKlB5QTDIV6S"
      },
      "source": [
        "# LoRA example\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-deepmind/gemma/blob/main/colabs/lora.ipynb)\n",
        "\n",
        "Example on using LoRA with Gemma (for both training and inference)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6fEKB1tISVW"
      },
      "outputs": [],
      "source": [
        "from etils import ecolab\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "# TODO(epot): Add open-source imports\n",
        "with ecolab.adhoc():\n",
        "  from gemma import gm\n",
        "  from gemma import peft  # Parameter fine-tuning module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kdAZkvOIryQ"
      },
      "source": [
        "## Initializing the model\n",
        "\n",
        "To use Gemma with LoRA, simply wrap any Gemma model in `gm.nn.LoRAWrapper`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-BbrzCVIupV"
      },
      "outputs": [],
      "source": [
        "model = gm.nn.LoRAWrapper(\n",
        "    rank=4,\n",
        "    model=gm.nn.Gemma2_2B(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI3Lg07SJff4"
      },
      "source": [
        "Initialize the weights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1shC1DpiJfsw"
      },
      "outputs": [],
      "source": [
        "token_ids = jnp.zeros((1, 256,), dtype=jnp.int32)  # Create the (batch_size, seq_length)\n",
        "\n",
        "params = model.init(\n",
        "    jax.random.key(0),\n",
        "    token_ids,\n",
        ")\n",
        "\n",
        "params = params['params']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3dWILqKKzG3"
      },
      "source": [
        "Inspect the params shape/structure. We can see LoRA weights have been added."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMq2Z9nXKcad"
      },
      "outputs": [],
      "source": [
        "with ecolab.collapse('Params'):\n",
        "  # p: Pretty-print, s: Array specs, h: Syntax highlighting\n",
        "  ecolab.disp(params, mode='psh')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGJl5YpKKOf-"
      },
      "source": [
        "Restore the pre-trained params. We use `peft.split_params` and `peft.merge_params` to replace the randomly initialized params with the pre-trained ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcO6oBuLKNjb"
      },
      "outputs": [],
      "source": [
        "# Splits the params into non-LoRA and LoRA weights\n",
        "original, lora = peft.split_params(params)\n",
        "\n",
        "# Load the params from the checkpoint\n",
        "# Providing the `params=original` ensure that:\n",
        "# * The memory from the old params is released (so only a single copy of the\n",
        "#   weights stays in memory)\n",
        "# * The restored params reuse the same sharding as the input (here there's no\n",
        "#   sharding, so isn't required)\n",
        "original = gm.ckpts.load_params(gm.ckpts.CheckpointPath.GEMMA2_2B_IT, params=original)\n",
        "\n",
        "# Merge the pretrained params back with LoRA\n",
        "params = peft.merge_params(original, lora)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jYRSpv2IwIY"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDfxP8YeI3iI"
      },
      "outputs": [],
      "source": [
        "# TODO(epot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvsQbQM4I4Cs"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Here's an example of running a single model call:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqU7a4eCI5Wr"
      },
      "outputs": [],
      "source": [
        "tokenizer = gm.text.Gemma2Tokenizer()\n",
        "\n",
        "prompt = tokenizer.encode('The capital of France is')\n",
        "prompt = jnp.asarray([tokenizer.special_tokens.BOS] + prompt)\n",
        "\n",
        "\n",
        "# Run the model\n",
        "out = model.apply(\n",
        "    {'params': params},\n",
        "    tokens=prompt,\n",
        "    return_last_only=True,  # Only predict the last token\n",
        ")\n",
        "\n",
        "\n",
        "# Show the token distribution\n",
        "tokenizer.plot_logits(out.logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dOSL9MHuMUa"
      },
      "source": [
        "To sample an entire sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ckwREdyqown"
      },
      "outputs": [],
      "source": [
        "sampler = gm.text.Sampler(\n",
        "    model=model,\n",
        "    params=params,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "sampler.sample('The capital of France is', max_new_tokens=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtAkaJTeuR0w"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {},
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
